{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import torch \n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Общие утилиты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = Path('imgs/finetune_gpt/')\n",
    "DATA_PATH = Path('data/finetune_gpt/')\n",
    "\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "DATA_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "def seed_all(seed: int) -> None:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    random.seed(seed)\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "seed_all(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка данных\n",
    "\n",
    "[Huggingface](https://huggingface.co/datasets/d0rj/geo-reviews-dataset-2023?row=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Загрузка датасета\n",
    "dataset = load_dataset(\"d0rj/geo-reviews-dataset-2023\", cache_dir=DATA_PATH / 'model_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразование данных в DataFrame\n",
    "data_df = pd.DataFrame(dataset['train'])\n",
    "\n",
    "print(\"Number of rows and columns in the data set:\", data_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_data = data_df.dropna(subset=['text', 'name_ru', 'rating'])\n",
    "work_data = work_data.drop_duplicates(subset=['text']).reset_index(drop=True)\n",
    "work_data['text'] = work_data['text'].str.replace('\\\\n', ' ')\n",
    "work_data = work_data[:15000]\n",
    "work_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_name_ru = work_data['name_ru'].unique().tolist()\n",
    "\n",
    "unique_rubrics = work_data['rubrics'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments\n",
    "from pathlib import Path\n",
    "\n",
    "class FineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name='ai-forever/rugpt3small_based_on_gpt2', \n",
    "                 cache_dir='model_cache',\n",
    "                 data_path=DATA_PATH):\n",
    "        self.data_path = Path(data_path)\n",
    "        \n",
    "        # Инициализация токенизатора и модели\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(model_name, cache_dir=str(self.data_path / cache_dir))\n",
    "\n",
    "    def prepare_data(self, df):\n",
    "        \"\"\"\n",
    "        Подготовка данных для обучения\n",
    "        \"\"\"\n",
    "        # Объединение типа проблемы и исходного текста в одну строку входных данных\n",
    "        df['input'] = df.apply(\n",
    "            lambda row: f\"<name_ru> {row['name_ru']} <rubrics> {row['rubrics']} <rating> {row['rating']} {self.tokenizer.eos_token}\", axis=1\n",
    "            )\n",
    "        \n",
    "        # Добавление к целевому тексту токена окончания строки\n",
    "        df['output'] = df.apply(lambda row: f\" <text> {row['text']} {self.tokenizer.eos_token}\", axis=1)\n",
    "        \n",
    "        # Подготовка пути для сохранения данных\n",
    "        dataset_path = self.data_path / 'train_dataset.txt'\n",
    "        # Запись данных в файл\n",
    "        with dataset_path.open('w', encoding='utf-8') as file:\n",
    "            for input_text, target_text in zip(df['input'], df['output']):\n",
    "                file.write(input_text + ' ' + target_text + '\\n')\n",
    "        return dataset_path\n",
    "\n",
    "    def fine_tune(self, \n",
    "                  dataset_path, \n",
    "                  output_name='fine_tuned_model', \n",
    "                  num_train_epochs=4, \n",
    "                  per_device_train_batch_size=4, \n",
    "                  learning_rate=5e-5, \n",
    "                  save_steps=10_000):\n",
    "        \"\"\"\n",
    "        Дообучение модели на заданном датасете.\n",
    "        \"\"\"\n",
    "        train_dataset = TextDataset(\n",
    "            tokenizer=self.tokenizer,\n",
    "            file_path=str(dataset_path),\n",
    "            block_size=256\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer, mlm=False\n",
    "        )\n",
    "\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=str(self.data_path / output_name),\n",
    "            overwrite_output_dir=True,\n",
    "            num_train_epochs=num_train_epochs,\n",
    "            per_device_train_batch_size=per_device_train_batch_size,\n",
    "            save_steps=save_steps,\n",
    "            learning_rate=learning_rate,\n",
    "            save_total_limit=2,\n",
    "            logging_dir=str(self.data_path / 'logs'),\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "        )\n",
    "\n",
    "        trainer.train()\n",
    "        # Сохранение обученной модели и токенизатора\n",
    "        self.model.save_pretrained(str(self.data_path / output_name))\n",
    "        self.tokenizer.save_pretrained(str(self.data_path / output_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pathlib import Path\n",
    "\n",
    "class TextGenerator:\n",
    "    def __init__(self, model_name='fine_tuned_model', data_path=DATA_PATH):\n",
    "        \"\"\"\n",
    "        Инициализация модели и токенизатора.\n",
    "        Загружаем модель и токенизатор из указанного пути.\n",
    "        \"\"\"\n",
    "        model_path = Path(data_path) / model_name\n",
    "        self.tokenizer = GPT2Tokenizer.from_pretrained(str(model_path))\n",
    "        self.model = GPT2LMHeadModel.from_pretrained(str(model_path))\n",
    "        self.model.eval()\n",
    "\n",
    "    def generate_text(self, \n",
    "                    name_ru: str, \n",
    "                    rubrics: str, \n",
    "                    rating: int,\n",
    "                    max_length=100, \n",
    "                    num_return_sequences=1, \n",
    "                    temperature=1.0, \n",
    "                    top_k=0, \n",
    "                    top_p=1.0, \n",
    "                    do_sample=False):\n",
    "        \"\"\"\n",
    "        Генерация текста на основе заданного начального текста (prompt) и параметров.\n",
    "        \n",
    "        Параметры:\n",
    "        - name_ru: Название организации.\n",
    "        - rubrics: Список рубрик, к которым относится организация.\n",
    "        - rating: Оценка пользователя.\n",
    "        - max_length: Максимальная длина сгенерированного текста.\n",
    "        - num_return_sequences: Количество возвращаемых последовательностей.\n",
    "        - temperature: Контролирует разнообразие вывода.\n",
    "        - top_k: Если больше 0, ограничивает количество слов для выборки только k наиболее вероятными словами.\n",
    "        - top_p: Если меньше 1.0, применяется nucleus sampling.\n",
    "        - do_sample: Если True, включает случайную выборку для увеличения разнообразия.\n",
    "        \"\"\"\n",
    "        # Формирование prompt\n",
    "        prompt_text = f\"<name_ru> {name_ru} <rubrics> {rubrics} <rating> {rating} {self.tokenizer.eos_token} <text> \"\n",
    "        \n",
    "        # Кодирование текста в формате, пригодном для модели\n",
    "        encoded_input = self.tokenizer.encode(prompt_text, return_tensors='pt')\n",
    "        \n",
    "        # Генерация текстов\n",
    "        outputs = self.model.generate(\n",
    "            encoded_input,\n",
    "            max_length=max_length + len(encoded_input[0]),\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            temperature=temperature,\n",
    "            top_k=top_k,\n",
    "            top_p=top_p,\n",
    "            do_sample=do_sample,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "        \n",
    "        # Декодирование результатов\n",
    "        all_texts = [self.tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "        \n",
    "        # Удаление входных данных из текстов\n",
    "        prompt_length = len(self.tokenizer.decode(encoded_input[0], skip_special_tokens=True))\n",
    "        trimmed_texts = [text[prompt_length:] for text in all_texts]\n",
    "        \n",
    "        # Возврат результатов в виде словаря\n",
    "        return {\n",
    "            \"full_texts\": all_texts,\n",
    "            \"generated_texts\": trimmed_texts\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuner = FineTuner()\n",
    "dataset_path = finetuner.prepare_data(work_data)\n",
    "finetuner.fine_tune(dataset_path, output_name='fine_tuned_model_gpt_2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Предикт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_name_ru[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_rubrics[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_ru = unique_name_ru[1]\n",
    "rubrics = unique_rubrics[1]\n",
    "rating = 1\n",
    "\n",
    "generator = TextGenerator(\n",
    "    model_name='fine_tuned_model_gpt_2',\n",
    "    data_path=DATA_PATH\n",
    ")\n",
    "generated_texts = generator.generate_text(\n",
    "    name_ru=name_ru,\n",
    "    rubrics=rubrics,\n",
    "    rating=rating,\n",
    "    max_length=100,\n",
    "    # num_beams=3 # если несколько последовательностей \n",
    "    num_return_sequences=3,\n",
    "    do_sample=True,\n",
    "    temperature=0.95,  # Слегка уменьшаем уверенность\n",
    "    top_k=10,         # Уменьшаем количество рассматриваемых верхних k слов\n",
    "    top_p=0.95        # Уменьшаем \"ядерность\" распределения\n",
    ")\n",
    "for i, text in enumerate(generated_texts['generated_texts']):\n",
    "    print(f\"Generated Text {i+1}: {text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
